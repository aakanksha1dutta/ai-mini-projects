Name: Aakanksha Dutta
email: adutta5@u.rochester.edu

This project impleements Logistic Regression algorithm from scratch. We update the weights in the direction of gradient of the Loss function. The Logistic loss or Cross Entropy Loss is the loss function we try to minimize. 

We take some steps in direction of this gradient. The size of this step is determined by the Learning Rate. Too small of a step size and the model takes too long to find optimal weights, and too big of a step size would mean we walk over the optimal weights.

After the optimal weights have been found, we then take the sigmoid of the dot product of the weights vector with each feature vector (wâ€¢x) corresponding to a datapoint. We assume a 50-50 threshold where if probability is less than 0.5, we assign the label 'negative' or -1 (in LIBSVM format) and 'positive' or +1 if p>0.5

The accuracy converges to 0.85 with this Logistic Regression model. There is overfitting in the model when learning rate = 0.01 and after the model has been trained for more than 10 epochs or iterations. (see pdf)


